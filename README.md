# MPD Value Estimation using Efficient Influence Functions EIF
This repository contains code for estimating the value of a target policy in a finite-horizon Markov Decision Process (MDP) using efficient influence functions (EIFs).

## Contents

* **mdp\_module.py**: Core Python module with:

  * Backward induction for Q and V computation
  * Forward recursion for state-action visitation probabilities
  * Importance weight (Î¼) calculation
  * Trajectory simulation under a behavior policy
  * EIF-based value estimation
* **notebooks/**: Jupyter notebooks for examples and plots.

## Requirements

* Python 3.7+
* NumPy
* Matplotlib

Install dependencies:

```bash
pip install numpy matplotlib
```

## Usage

1. **Save** `mdp_module.py` next to your notebook or script.
2. **Import** and set up an MDP:

   ```python
   import numpy as np
   from mdp_module import MDP, run_simulation

   # Example MDP
   num_states, num_actions, T, s0 = 3, 2, 100, 0
   P = np.zeros((3,2,3))
   P[0,0] = [0.7,0.3,0.0]; P[0,1] = [0.0,0.4,0.6]
   P[1,0] = [0.8,0.0,0.2]; P[1,1] = [0.0,0.5,0.5]
   P[2,:,2] = 1.0
   R = np.zeros((3,2)); R[0] = [5.0, 1.0]
   pi_b = np.array([[0.3,0.7],[0.8,0.2],[0.1,0.9]])
   pi_e = np.array([[0.5,0.5],[0.0,1.0],[0.0,1.0]])
   mdp = MDP(3,2,100,0,P,R,pi_b,pi_e)
   ```
3. **Estimate** policy value:

   ```python
   eif_vals, term_vals = run_simulation(mdp, n_trajectories=1000, seed=42)
   ```
4. **Plot** results:

   ```python
   import matplotlib.pyplot as plt
   plt.hist(eif_vals, bins=30)
   plt.show()
   ```

